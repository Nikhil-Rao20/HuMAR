{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67eb17d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Args configured with backbone: mobilevit_xxs and transformer: fully_conv_optim\n"
     ]
    }
   ],
   "source": [
    "class UniPHDArgs:\n",
    "    def __init__(self):\n",
    "        # -----------------------------\n",
    "        # Config / Override Parameters\n",
    "        # -----------------------------\n",
    "        self.config_file = \"\"           # REQUIRED: path to .py config\n",
    "        self.options = None             # List of overrides via DictAction (e.g., [\"lr=1e-4\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # Prompt / Text Encoder\n",
    "        # -----------------------------\n",
    "        self.freeze_text_encoder = False\n",
    "        self.train_trigger = \"text scribble point\"\n",
    "        self.eval_trigger = \"text\"\n",
    "\n",
    "        self.kps_visi_trigger = True\n",
    "        self.pose_guide_trigger = False\n",
    "        self.late_within_attn_trigger = True\n",
    "        self.within_type = \"attn_graph\"\n",
    "        self.no_mask = False\n",
    "\n",
    "        # -----------------------------\n",
    "        # Model Backbone\n",
    "        # -----------------------------\n",
    "        self.backbone = \"mobilevit_xxs\"\n",
    "        self.swin_pretrain_path = r\"C:\\Users\\nikhi\\Desktop\\HuMAR\\datasets\\RefHuman\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # Dataset Parameters\n",
    "        # -----------------------------\n",
    "        self.dataset_file = \"refhuman\"\n",
    "        self.coco_path = \"../datasets/RefHuman\"\n",
    "        self.remove_difficult = False\n",
    "\n",
    "        # -----------------------------\n",
    "        # Training Parameters\n",
    "        # -----------------------------\n",
    "        self.output_dir = \"./results/UniPHD_Results\"\n",
    "        self.note = \"\"\n",
    "        self.device = \"cuda\"\n",
    "        self.seed = 42\n",
    "\n",
    "        self.resume = \"\"                # checkpoint path\n",
    "        self.pretrain_model_path = None # external checkpoint\n",
    "        self.finetune_ignore = None     # list[str]\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.eval = False\n",
    "        self.num_workers = 0\n",
    "        self.find_unused_params = False\n",
    "        self.save_log = False\n",
    "\n",
    "        # -----------------------------\n",
    "        # Distributed Training\n",
    "        # -----------------------------\n",
    "        self.world_size = 1\n",
    "        self.dist_url = \"env://\"\n",
    "        self.rank = 0\n",
    "        self.local_rank = 0\n",
    "        self.amp = False                # Mixed precision\n",
    "\n",
    "        # -----------------------------\n",
    "        # Additional Keys Updated Later\n",
    "        # -----------------------------\n",
    "        self.use_ema = False\n",
    "        self.debug = False\n",
    "\n",
    "        # -----------------------------\n",
    "        # MODEL NAME (REQUIRED FOR build_model_main)\n",
    "        # -----------------------------\n",
    "        # MUST be set to something like \"UniPHD\", \"UniRef\", \"UniSeg\", etc.\n",
    "        self.modelname = \"uniphd\"             # <---- you MUST fill this\n",
    "        self.num_classes = 2\n",
    "\n",
    "        self.lr = 0.0001\n",
    "        self.lr_adjacent_matrix = 1e-04\n",
    "        self.param_dict_type = 'default'\n",
    "        self.lr_backbone = 1e-05\n",
    "        self.lr_backbone_names = ['backbone.0']\n",
    "        self.lr_linear_proj_names = ['reference_points', 'sampling_offsets']\n",
    "        self.lr_linear_proj_mult = 0.1\n",
    "        self.lr_text_encoder = 0.0001\n",
    "        self.lr_text_encoder_names = ['text_encoder']\n",
    "        self.batch_size = 4\n",
    "        self.weight_decay = 0.0001\n",
    "        self.epochs = 20\n",
    "        self.lr_drop = 18\n",
    "        self.save_checkpoint_interval = 5\n",
    "        self.clip_max_norm = 0.1\n",
    "\n",
    "        self.modelname = 'uniphd'\n",
    "        self.frozen_weights = None\n",
    "        self.use_checkpoint = False\n",
    "        self.dilation = False\n",
    "        self.position_embedding = 'sine'\n",
    "        self.pe_temperatureH = 20\n",
    "        self.pe_temperatureW = 20\n",
    "        self.return_interm_indices = [0, 1, 2, 3]\n",
    "        self.backbone_freeze_keywords = None\n",
    "\n",
    "        # for transformer\n",
    "        self.transformer_type = 'fully_conv_optim'  # 'original', 'efficient', 'fully_conv', 'fully_conv_optim'\n",
    "        self.hidden_dim = 256\n",
    "        self.dropout = 0.0\n",
    "        self.dim_feedforward = 2048\n",
    "        self.enc_layers = 6\n",
    "        self.dec_layers = 6\n",
    "        self.pre_norm = False\n",
    "        self.return_intermediate_dec = True\n",
    "        self.enc_n_points = 4\n",
    "        self.dec_n_points = 4\n",
    "        self.learnable_tgt_init = False\n",
    "        self.transformer_activation = 'relu'\n",
    "\n",
    "        # for main model\n",
    "        self.num_classes=2\n",
    "        self.nheads = 8\n",
    "        self.num_queries = 20\n",
    "        self.num_feature_levels = 4\n",
    "        self.dec_pred_class_embed_share = False\n",
    "        self.dec_pred_pose_embed_share = False\n",
    "        self.two_stage_type = 'standard'\n",
    "        self.two_stage_bbox_embed_share = False\n",
    "        self.two_stage_class_embed_share = False\n",
    "        self.cls_no_bias = False\n",
    "        self.num_body_points = 17\n",
    "\n",
    "        # for loss\n",
    "        self.focal_alpha = 0.25\n",
    "        self.cls_loss_coef = 2.0\n",
    "        self.bbox_loss_coef = 5.0\n",
    "        self.keypoints_loss_coef = 10.0\n",
    "        self.keypoints_visi_loss_coef = 4.0\n",
    "        self.oks_loss_coef=4.0\n",
    "        self.giou_loss_coef = 2.0\n",
    "        self.enc_loss_coef = 1.0\n",
    "        self.interm_loss_coef = 1.0\n",
    "        self.mask_loss_coef = 2.0\n",
    "        self.dice_loss_coef = 5.0\n",
    "        self.no_interm_loss = False\n",
    "        self.aux_loss = True\n",
    "\n",
    "        # for matcher\n",
    "        self.matcher_type = 'HungarianMatcher'\n",
    "        self.set_cost_class = 2.0\n",
    "        self.set_cost_bbox = 5.0\n",
    "        self.set_cost_giou = 2.0\n",
    "        self.set_cost_keypoints = 10.0\n",
    "        self.set_cost_keypoints_visi = 4.0\n",
    "        self.set_cost_oks=4.0\n",
    "        self.set_cost_kpvis = 0.0\n",
    "        self.set_cost_mask = 2.0\n",
    "        self.set_cost_dice = 5.0\n",
    "\n",
    "        # for postprocess\n",
    "        self.num_select = 20\n",
    "\n",
    "        # for ema\n",
    "        self.use_ema = False\n",
    "        self.ema_decay = 0.9997\n",
    "        self.ema_epoch = 0\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"For clean printing.\"\"\"\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in self.__dict__.items()])\n",
    "\n",
    "\n",
    "args = UniPHDArgs()\n",
    "args.config_file = \"configs/uniphd.py\"\n",
    "args.modelname = \"uniphd\"\n",
    "print(f\"âœ“ Args configured with backbone: {args.backbone} and transformer: {args.transformer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ae61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_main():\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    assert 'uniphd' in MODULE_BUILD_FUNCS._module_dict\n",
    "    build_func = MODULE_BUILD_FUNCS.get('uniphd')\n",
    "    model, criterion, postprocessors = build_func(args)\n",
    "    return model, criterion, postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3411af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniLM: 22.7\n",
    "# TinyBERT: 14.5\n",
    "# ALBERTA: 11.6\n",
    "# MobileBERT: 24.6\n",
    "# DistilBERT: 66.3\n",
    "# Roberta: 124.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b12c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mobilevit_xxs backbone\n",
      "Output channels per stage: [16, 24, 48, 64]\n",
      "Backbone Summary:  ================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "Joiner                                                  --\n",
      "â”œâ”€MobileViTBackbone: 1-1                                --\n",
      "â”‚    â””â”€FeatureListNet: 2-1                              --\n",
      "â”‚    â”‚    â””â”€ConvNormAct: 3-1                            464\n",
      "â”‚    â”‚    â””â”€Sequential: 3-2                             1,472\n",
      "â”‚    â”‚    â””â”€Sequential: 3-3                             7,696\n",
      "â”‚    â”‚    â””â”€Sequential: 3-4                             139,888\n",
      "â”‚    â”‚    â””â”€Sequential: 3-5                             341,824\n",
      "â”œâ”€PositionEmbeddingSineHW: 1-2                          --\n",
      "================================================================================\n",
      "Total params: 491,344\n",
      "Trainable params: 491,344\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "Using Optimized FULLY CONVOLUTIONAL Transfomer\n",
      "********** Enabling Text Prompt ***************\n",
      "\n",
      "Use ALBERT as text encoder. Freeze: False\n",
      "********** Enabling Positional Prompt ***************\n",
      "\n",
      "âœ“ Model loaded on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model, criterion, postprocessors = build_model_main()\n",
    "\n",
    "# Move model to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"âœ“ Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "571d59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully_conv_optim: 3.8\n",
    "# fully_conv: 10.0\n",
    "# optimizedL 8.7\n",
    "# original: 25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5f2081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Input prepared: images shape torch.Size([1, 3, 256, 256]), 1 targets\n",
      "âœ“ Keypoints shape: torch.Size([1, 51])\n",
      "âœ“ Device: cuda:0\n",
      "\n",
      "======================================================================\n",
      "â±ï¸  INFERENCE TIMING BREAKDOWN (ms)\n",
      "======================================================================\n",
      "======================================================================\n",
      "TOTAL FORWARD                              319.38 ms  (100.00%)\n",
      "======================================================================\n",
      "Backbone                          28.64 ms  (  9.0%) â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Text Encoder                      61.67 ms  ( 19.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Text Pos Encoding                  0.00 ms  (  0.0%) â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Multimodal Fusion                  6.65 ms  (  2.1%) â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Transformer                      103.74 ms  ( 32.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Prediction Heads                   8.05 ms  (  2.5%) â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Hungarian Matching                67.85 ms  ( 21.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Mask Prediction                   42.78 ms  ( 13.4%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "Output Prep                      110.63 ms  ( 34.6%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
      "======================================================================\n",
      "\n",
      "\n",
      "âœ… Forward pass successful with mobilevit_xxs backbone!\n",
      "âœ“ Output keys: ['pred_logits', 'pred_keypoints', 'pred_keypoints_visi', 'pred_boxes', 'main_indices', 'aux_outputs', 'aux_indices', 'pred_masks', 'padded_hw']\n",
      "âœ“ Predicted boxes shape: torch.Size([1, 20, 4])\n",
      "âœ“ Predicted keypoints shape: torch.Size([1, 20, 51])\n",
      "âœ“ Predicted logits shape: torch.Size([1, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from util.misc import nested_tensor_from_tensor_list\n",
    "\n",
    "# Create dummy images (batch_size=2, 3 channels, 256x256)\n",
    "dummy_images = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Create complete dummy targets with all required fields\n",
    "num_keypoints = 17  # COCO format has 17 keypoints\n",
    "dummy_targets = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Create keypoints: [num_instances, num_keypoints * 3] where 3 = (x, y, visibility)\n",
    "    # Flattened format: [x1, y1, v1, x2, y2, v2, ...]\n",
    "    keypoints_flat = torch.rand(1, num_keypoints * 3)  # Random values between 0 and 1\n",
    "    \n",
    "    target = {\n",
    "        'caption': 'A person standing' if i == 0 else 'A person sitting',\n",
    "        'labels': torch.tensor([1], dtype=torch.long),  # Class labels (1 = person)\n",
    "        'boxes': torch.tensor([[0.5, 0.5, 0.3, 0.4]], dtype=torch.float32),  # [cx, cy, w, h] normalized\n",
    "        'keypoints': keypoints_flat,  # [num_instances, num_keypoints * 3] flattened\n",
    "        'area': torch.tensor([0.12], dtype=torch.float32),  # Area of bbox\n",
    "        'iscrowd': torch.tensor([0], dtype=torch.long),  # Not a crowd\n",
    "        'orig_size': torch.tensor([256, 256], dtype=torch.long),  # Original image size\n",
    "        'size': torch.tensor([256, 256], dtype=torch.long),  # Current image size\n",
    "        'scribble': torch.rand(8, 2)  # 8 scribble points with (x, y) coordinates\n",
    "    }\n",
    "    dummy_targets.append(target)\n",
    "\n",
    "# Move everything to CUDA\n",
    "device = next(model.parameters()).device\n",
    "dummy_images = dummy_images.to(device)\n",
    "\n",
    "# Move all target tensors to device\n",
    "for target in dummy_targets:\n",
    "    for k, v in target.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            target[k] = v.to(device)\n",
    "\n",
    "# Create NestedTensor from list of images\n",
    "samples = nested_tensor_from_tensor_list(list(dummy_images))\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ“ Input prepared: images shape {dummy_images.shape}, {len(dummy_targets)} targets\")\n",
    "print(f\"âœ“ Keypoints shape: {dummy_targets[0]['keypoints'].shape}\")\n",
    "print(f\"âœ“ Device: {device}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(samples, dummy_targets)\n",
    "\n",
    "print(\"\\nâœ… Forward pass successful with mobilevit_xxs backbone!\")\n",
    "print(f\"âœ“ Output keys: {list(outputs.keys())}\")\n",
    "print(f\"âœ“ Predicted boxes shape: {outputs['pred_boxes'].shape}\")\n",
    "print(f\"âœ“ Predicted keypoints shape: {outputs['pred_keypoints'].shape}\")\n",
    "print(f\"âœ“ Predicted logits shape: {outputs['pred_logits'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5665d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All configuration options documented above\n",
      "\n",
      "Current configuration:\n",
      "  Backbone: mobilevit_xxs\n",
      "  Transformer: efficient\n",
      "  Text Encoder: MiniLM (to change, edit text_encoder.py)\n",
      "\n",
      "ðŸ’¡ Recommended for RTX 4050 6GB:\n",
      "  backbone='mobilevit_xxs' + transformer='fully_conv_optim' + text='TinyBERT'\n",
      "  Total: ~20M params\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ALL AVAILABLE MODEL OPTIONS\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "BACKBONE OPTIONS:\n",
    "-----------------\n",
    "MobileViT (Ultra-lightweight):\n",
    "  - mobilevit_xxs    (1.3M params)  â­ LIGHTEST\n",
    "  - mobilevit_xs     (2.3M params)\n",
    "  - mobilevit_s      (5.6M params)\n",
    "\n",
    "SegFormer (Efficient hierarchical ViT):\n",
    "  - segformer_mit_b0 (3.7M params)  â­ BEST EFFICIENCY\n",
    "  - segformer_mit_b1 (13.7M params)\n",
    "\n",
    "EfficientFormerV2 (State-of-the-art efficient ViT):\n",
    "  - efficientformerv2_s0\n",
    "  - efficientformerv2_s1\n",
    "  - efficientformerv2_s2\n",
    "\n",
    "PoolFormer (MetaFormer with pooling):\n",
    "  - poolformer_s12   (12M params)\n",
    "  - poolformer_s24\n",
    "  - poolformer_s36\n",
    "\n",
    "Swin Transformer (Original):\n",
    "  - swin_T_224_1k    (28M params)  â­ ORIGINAL DEFAULT\n",
    "\n",
    "\n",
    "TRANSFORMER OPTIONS:\n",
    "--------------------\n",
    "  - 'original'        (27M params)   - Deformable attention, highest quality\n",
    "  - 'efficient'       (11M params)   - Linear attention O(N), Conv+Attention\n",
    "  - 'fully_conv'      (12M params)   - Pure convolution, ConvNeXt blocks\n",
    "  - 'fully_conv_optim'(2-3M params)  - Ghost modules, ultra-lightweight â­\n",
    "\n",
    "\n",
    "TEXT ENCODER OPTIONS:\n",
    "---------------------\n",
    "(Edit line 42 in models/uniphd/text_encoder/text_encoder.py)\n",
    "  - 'MiniLM'     (23M, 384-dim)   â­ DEFAULT - Best for sentence embeddings\n",
    "  - 'TinyBERT'   (14.5M, 312-dim) â­ LIGHTEST\n",
    "  - 'DistilBERT' (66M, 768-dim)   - Balanced\n",
    "  - 'TiTeLATE'   (768-dim)        - Information retrieval\n",
    "  - 'TinyBERT'   (14.5M, 312-dim) â­ LIGHTEST - General distilled BERT\n",
    "  - 'ALBERT'     (11.8M, 768-dim) - Shared-parameter BERT, very efficient\n",
    "  - 'MobileBERT' (25M, 512-dim)   - Optimized for mobile devices\n",
    "  - 'DistilBERT' (66M, 768-dim)   - Balanced\n",
    "  - 'TiTeLATE'   (768-dim)        - Information retrieval\n",
    "  - 'Roberta'    (125M, 768-dim)  - Original, maximum understanding\n",
    "\n",
    "\n",
    "USAGE:\n",
    "------\n",
    "Change in cell 1:\n",
    "  args.backbone = \"mobilevit_xxs\"           # Pick any backbone\n",
    "  args.transformer_type = 'original'         # Pick any transformer\n",
    "\n",
    "For text encoder:\n",
    "  Edit: models/uniphd/text_encoder/text_encoder.py line 42\n",
    "  Change: self.text_backbone_name = \"MiniLM\"  # to any option above\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ“ All configuration options documented above\")\n",
    "print(f\"\\nCurrent configuration:\")\n",
    "print(f\"  Backbone: {args.backbone}\")\n",
    "print(f\"  Transformer: {args.transformer_type}\")\n",
    "print(f\"  Text Encoder: MiniLM (to change, edit text_encoder.py)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8e092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ALL BACKBONE VARIANTS\n",
      "================================================================================\n",
      "\n",
      "Total backbones to test: 15\n",
      "\n",
      "\n",
      "[1/15] Testing: mobilevit_xxs\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 20.71M (20,714,474)\n",
      "Info: Output channels per stage: [16, 24, 48, 64]\n",
      "\n",
      "[2/15] Testing: mobilevit_xs\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 21.34M (21,338,970)\n",
      "Info: Output channels per stage: [32, 48, 64, 80]\n",
      "\n",
      "[3/15] Testing: mobilevit_s\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 22.95M (22,949,610)\n",
      "Info: Output channels per stage: [32, 64, 96, 128]\n",
      "\n",
      "[4/15] Testing: segformer_mit_b0\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 24.11M (24,109,690)\n",
      "Info: Output channels per stage: [32, 64, 160, 256]\n",
      "\n",
      "[5/15] Testing: segformer_mit_b1\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 34.71M (34,709,210)\n",
      "Info: Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "[6/15] Testing: efficientformerv2_s0\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 23.79M (23,792,234)\n",
      "Info: Output channels per stage: [32, 48, 96, 176]\n",
      "\n",
      "[7/15] Testing: efficientformerv2_s1\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 26.42M (26,416,610)\n",
      "Info: Output channels per stage: [32, 48, 120, 224]\n",
      "\n",
      "[8/15] Testing: efficientformerv2_s2\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 33.00M (32,995,562)\n",
      "Info: Output channels per stage: [32, 64, 144, 288]\n",
      "\n",
      "[9/15] Testing: efficientformer_l1\n",
      "------------------------------------------------------------\n",
      "Status: âŒ FAILED\n",
      "Error: Unknown backbone efficientformer_l1\n",
      "\n",
      "[10/15] Testing: efficientformer_l3\n",
      "------------------------------------------------------------\n",
      "Status: âŒ FAILED\n",
      "Error: Unknown backbone efficientformer_l3\n",
      "\n",
      "[11/15] Testing: efficientformer_l7\n",
      "------------------------------------------------------------\n",
      "Status: âŒ FAILED\n",
      "Error: Unknown backbone efficientformer_l7\n",
      "\n",
      "[12/15] Testing: poolformer_s12\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 32.96M (32,958,938)\n",
      "Info: Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "[13/15] Testing: poolformer_s24\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 42.43M (42,432,730)\n",
      "Info: Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "[14/15] Testing: poolformer_s36\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 51.91M (51,906,522)\n",
      "Info: Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "[15/15] Testing: swin_T_224_1k\n",
      "------------------------------------------------------------\n",
      "Status: âœ… SUCCESS\n",
      "Trainable Parameters: 49.80M (49,796,820)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ALL BACKBONE TESTS\n",
      "================================================================================\n",
      "\n",
      "âœ… Successful: 12/15\n",
      "âš ï¸  Fallbacks:  0/15\n",
      "âŒ Failed:     3/15\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DETAILED RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… SUCCESS mobilevit_xxs\n",
      "    Trainable Parameters: 20.71M\n",
      "    Output channels per stage: [16, 24, 48, 64]\n",
      "\n",
      "âœ… SUCCESS mobilevit_xs\n",
      "    Trainable Parameters: 21.34M\n",
      "    Output channels per stage: [32, 48, 64, 80]\n",
      "\n",
      "âœ… SUCCESS mobilevit_s\n",
      "    Trainable Parameters: 22.95M\n",
      "    Output channels per stage: [32, 64, 96, 128]\n",
      "\n",
      "âœ… SUCCESS segformer_mit_b0\n",
      "    Trainable Parameters: 24.11M\n",
      "    Output channels per stage: [32, 64, 160, 256]\n",
      "\n",
      "âœ… SUCCESS segformer_mit_b1\n",
      "    Trainable Parameters: 34.71M\n",
      "    Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "âœ… SUCCESS efficientformerv2_s0\n",
      "    Trainable Parameters: 23.79M\n",
      "    Output channels per stage: [32, 48, 96, 176]\n",
      "\n",
      "âœ… SUCCESS efficientformerv2_s1\n",
      "    Trainable Parameters: 26.42M\n",
      "    Output channels per stage: [32, 48, 120, 224]\n",
      "\n",
      "âœ… SUCCESS efficientformerv2_s2\n",
      "    Trainable Parameters: 33.00M\n",
      "    Output channels per stage: [32, 64, 144, 288]\n",
      "\n",
      "âŒ FAILED efficientformer_l1\n",
      "    Error: Unknown backbone efficientformer_l1\n",
      "\n",
      "âŒ FAILED efficientformer_l3\n",
      "    Error: Unknown backbone efficientformer_l3\n",
      "\n",
      "âŒ FAILED efficientformer_l7\n",
      "    Error: Unknown backbone efficientformer_l7\n",
      "\n",
      "âœ… SUCCESS poolformer_s12\n",
      "    Trainable Parameters: 32.96M\n",
      "    Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "âœ… SUCCESS poolformer_s24\n",
      "    Trainable Parameters: 42.43M\n",
      "    Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "âœ… SUCCESS poolformer_s36\n",
      "    Trainable Parameters: 51.91M\n",
      "    Output channels per stage: [64, 128, 320, 512]\n",
      "\n",
      "âœ… SUCCESS swin_T_224_1k\n",
      "    Trainable Parameters: 49.80M\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED WORKING BACKBONES (sorted by parameters):\n",
      "================================================================================\n",
      "  â€¢ mobilevit_xxs           20.71M params\n",
      "  â€¢ mobilevit_xs            21.34M params\n",
      "  â€¢ mobilevit_s             22.95M params\n",
      "  â€¢ efficientformerv2_s0    23.79M params\n",
      "  â€¢ segformer_mit_b0        24.11M params\n",
      "  â€¢ efficientformerv2_s1    26.42M params\n",
      "  â€¢ poolformer_s12          32.96M params\n",
      "  â€¢ efficientformerv2_s2    33.00M params\n",
      "  â€¢ segformer_mit_b1        34.71M params\n",
      "  â€¢ poolformer_s24          42.43M params\n",
      "\n",
      "âœ“ Test complete. Restored backbone to: mobilevit_xxs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BACKBONE COMPATIBILITY TEST - Testing all backbone variants\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# List of all backbones to test (excluding Swin variants except default)\n",
    "backbones_to_test = [\n",
    "    # MobileViT variants\n",
    "    'mobilevit_xxs',\n",
    "    'mobilevit_xs',\n",
    "    'mobilevit_s',\n",
    "    \n",
    "    # SegFormer variants\n",
    "    'segformer_mit_b0',\n",
    "    'segformer_mit_b1',\n",
    "    \n",
    "    # EfficientFormer variants\n",
    "    'efficientformerv2_s0',\n",
    "    'efficientformerv2_s1',\n",
    "    'efficientformerv2_s2',\n",
    "    'efficientformer_l1',\n",
    "    'efficientformer_l3',\n",
    "    'efficientformer_l7',\n",
    "    \n",
    "    # PoolFormer variants\n",
    "    'poolformer_s12',\n",
    "    'poolformer_s24',\n",
    "    'poolformer_s36',\n",
    "    \n",
    "    # Swin default only\n",
    "    'swin_T_224_1k',\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING ALL BACKBONE VARIANTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal backbones to test: {len(backbones_to_test)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def build_model_main_backbone(args):\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    assert 'uniphd' in MODULE_BUILD_FUNCS._module_dict\n",
    "    build_func = MODULE_BUILD_FUNCS.get('uniphd')\n",
    "    model, criterion, postprocessors = build_func(args)\n",
    "    return model, criterion, postprocessors\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, backbone_name in enumerate(backbones_to_test, 1):\n",
    "    print(f\"\\n[{i}/{len(backbones_to_test)}] Testing: {backbone_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Update args with new backbone\n",
    "        args.backbone = backbone_name\n",
    "        \n",
    "        # Capture stdout to detect fallback messages\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = captured_output = StringIO()\n",
    "        \n",
    "        # Try to build the model\n",
    "        # from models.registry import MODULE_BUILD_FUNCS\n",
    "        # build_func = MODULE_BUILD_FUNCS.get('uniphd')\n",
    "        test_model, _, _ = build_model_main_backbone(args)\n",
    "        \n",
    "        # Restore stdout\n",
    "        sys.stdout = old_stdout\n",
    "        output = captured_output.getvalue()\n",
    "        \n",
    "        # Check if there was a fallback\n",
    "        fallback = None\n",
    "        if \"Falling back\" in output or \"falling back\" in output.lower():\n",
    "            # Extract fallback model name\n",
    "            for line in output.split('\\n'):\n",
    "                if 'falling back' in line.lower() or 'loaded' in line.lower():\n",
    "                    fallback = line.strip()\n",
    "                    break\n",
    "        \n",
    "        # Check output channels\n",
    "        channels_info = None\n",
    "        for line in output.split('\\n'):\n",
    "            if 'Output channels' in line:\n",
    "                channels_info = line.strip()\n",
    "                break\n",
    "        \n",
    "        status = \"âœ… SUCCESS\"\n",
    "        if fallback:\n",
    "            status = f\"âš ï¸ FALLBACK\"\n",
    "        \n",
    "        # Count parameters using torchinfo\n",
    "        from torchinfo import summary\n",
    "        try:\n",
    "            model_stats = summary(test_model, verbose=0)\n",
    "            total_params = model_stats.total_params\n",
    "            trainable_params = model_stats.trainable_params\n",
    "            params_millions = trainable_params / 1_000_000\n",
    "        except:\n",
    "            total_params = sum(p.numel() for p in test_model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "            params_millions = trainable_params / 1_000_000\n",
    "        \n",
    "        results.append({\n",
    "            'backbone': backbone_name,\n",
    "            'status': status,\n",
    "            'channels': channels_info,\n",
    "            'fallback': fallback,\n",
    "            'params_M': params_millions,\n",
    "            'trainable_params': trainable_params\n",
    "        })\n",
    "        \n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Trainable Parameters: {params_millions:.2f}M ({trainable_params:,})\")\n",
    "        if channels_info:\n",
    "            print(f\"Info: {channels_info}\")\n",
    "        if fallback:\n",
    "            print(f\"Fallback: {fallback}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        sys.stdout = old_stdout\n",
    "        error_msg = str(e)\n",
    "        if len(error_msg) > 100:\n",
    "            error_msg = error_msg[:100] + \"...\"\n",
    "        \n",
    "        results.append({\n",
    "            'backbone': backbone_name,\n",
    "            'status': \"âŒ FAILED\",\n",
    "            'channels': None,\n",
    "            'fallback': None,\n",
    "            'error': error_msg\n",
    "        })\n",
    "        \n",
    "        print(f\"Status: âŒ FAILED\")\n",
    "        print(f\"Error: {error_msg}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY OF ALL BACKBONE TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "success_count = sum(1 for r in results if \"SUCCESS\" in r['status'])\n",
    "fallback_count = sum(1 for r in results if \"FALLBACK\" in r['status'])\n",
    "failed_count = sum(1 for r in results if \"FAILED\" in r['status'])\n",
    "\n",
    "print(f\"\\nâœ… Successful: {success_count}/{len(backbones_to_test)}\")\n",
    "print(f\"âš ï¸  Fallbacks:  {fallback_count}/{len(backbones_to_test)}\")\n",
    "print(f\"âŒ Failed:     {failed_count}/{len(backbones_to_test)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DETAILED RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n{result['status']} {result['backbone']}\")\n",
    "    if result.get('params_M'):\n",
    "        print(f\"    Trainable Parameters: {result['params_M']:.2f}M\")\n",
    "    if result.get('channels'):\n",
    "        print(f\"    {result['channels']}\")\n",
    "    if result.get('fallback'):\n",
    "        print(f\"    Fallback: {result['fallback']}\")\n",
    "    if result.get('error'):\n",
    "        print(f\"    Error: {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDED WORKING BACKBONES (sorted by parameters):\")\n",
    "print(\"=\" * 80)\n",
    "working = [r for r in results if \"SUCCESS\" in r['status'] or \"FALLBACK\" in r['status']]\n",
    "working_sorted = sorted(working, key=lambda x: x.get('params_M', 999))\n",
    "for r in working_sorted[:10]:  # Show top 10\n",
    "    params_str = f\"{r['params_M']:.2f}M\" if r.get('params_M') else \"N/A\"\n",
    "    print(f\"  â€¢ {r['backbone']:20s}  {params_str:>8s} params\")\n",
    "\n",
    "# Restore original backbone\n",
    "args.backbone = \"mobilevit_xxs\"\n",
    "print(f\"\\nâœ“ Test complete. Restored backbone to: {args.backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8bb44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FPS BENCHMARK - INFERENCE SPEED TEST\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Batch size: 1\n",
      "  Image size: 640x640\n",
      "  Warmup runs: 5\n",
      "  Test runs: 30\n",
      "  Caption length: 4-5 words\n",
      "\n",
      "\n",
      "Testing 12 working backbones...\n",
      "\n",
      "[1/12] Testing: mobilevit_xxs\n",
      "------------------------------------------------------------\n",
      "Loaded mobilevit_xxs backbone\n",
      "Output channels per stage: [16, 24, 48, 64]\n",
      "Backbone Summary:  ================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "Joiner                                                  --\n",
      "â”œâ”€MobileViTBackbone: 1-1                                --\n",
      "â”‚    â””â”€FeatureListNet: 2-1                              --\n",
      "â”‚    â”‚    â””â”€ConvNormAct: 3-1                            464\n",
      "â”‚    â”‚    â””â”€Sequential: 3-2                             1,472\n",
      "â”‚    â”‚    â””â”€Sequential: 3-3                             7,696\n",
      "â”‚    â”‚    â””â”€Sequential: 3-4                             139,888\n",
      "â”‚    â”‚    â””â”€Sequential: 3-5                             341,824\n",
      "â”œâ”€PositionEmbeddingSineHW: 1-2                          --\n",
      "================================================================================\n",
      "Total params: 491,344\n",
      "Trainable params: 491,344\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "Using Optimized FULLY CONVOLUTIONAL Transfomer\n",
      "********** Enabling Text Prompt ***************\n",
      "\n",
      "Use ALBERT as text encoder. Freeze: False\n",
      "********** Enabling Positional Prompt ***************\n",
      "\n",
      "  Warming up (5 iterations)... Done\n",
      "  Running benchmark (30 iterations)... Done\n",
      "  âœ… FPS: 5.10  |  196.1 ms/image\n",
      "\n",
      "[2/12] Testing: mobilevit_xs\n",
      "------------------------------------------------------------\n",
      "Loaded mobilevit_xs backbone\n",
      "Output channels per stage: [32, 48, 64, 80]\n",
      "Backbone Summary:  ================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "Joiner                                                  --\n",
      "â”œâ”€MobileViTBackbone: 1-1                                --\n",
      "â”‚    â””â”€FeatureListNet: 2-1                              --\n",
      "â”‚    â”‚    â””â”€ConvNormAct: 3-1                            464\n",
      "â”‚    â”‚    â””â”€Sequential: 3-2                             3,968\n",
      "â”‚    â”‚    â””â”€Sequential: 3-3                             54,048\n",
      "â”‚    â”‚    â””â”€Sequential: 3-4                             297,152\n",
      "â”‚    â”‚    â””â”€Sequential: 3-5                             699,152\n",
      "â”œâ”€PositionEmbeddingSineHW: 1-2                          --\n",
      "================================================================================\n",
      "Total params: 1,054,784\n",
      "Trainable params: 1,054,784\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "Using Optimized FULLY CONVOLUTIONAL Transfomer\n",
      "********** Enabling Text Prompt ***************\n",
      "\n",
      "Use ALBERT as text encoder. Freeze: False\n",
      "********** Enabling Positional Prompt ***************\n",
      "\n",
      "  Warming up (5 iterations)... Done\n",
      "  Running benchmark (30 iterations)... Done\n",
      "  âœ… FPS: 4.74  |  211.2 ms/image\n",
      "\n",
      "[3/12] Testing: mobilevit_s\n",
      "------------------------------------------------------------\n",
      "Loaded mobilevit_s backbone\n",
      "Output channels per stage: [32, 64, 96, 128]\n",
      "Backbone Summary:  ================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "Joiner                                                  --\n",
      "â”œâ”€MobileViTBackbone: 1-1                                --\n",
      "â”‚    â””â”€FeatureListNet: 2-1                              --\n",
      "â”‚    â”‚    â””â”€ConvNormAct: 3-1                            464\n",
      "â”‚    â”‚    â””â”€Sequential: 3-2                             3,968\n",
      "â”‚    â”‚    â””â”€Sequential: 3-3                             86,528\n",
      "â”‚    â”‚    â””â”€Sequential: 3-4                             656,768\n",
      "â”‚    â”‚    â””â”€Sequential: 3-5                             1,772,032\n",
      "â”œâ”€PositionEmbeddingSineHW: 1-2                          --\n",
      "================================================================================\n",
      "Total params: 2,519,760\n",
      "Trainable params: 2,519,760\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "Using Optimized FULLY CONVOLUTIONAL Transfomer\n",
      "********** Enabling Text Prompt ***************\n",
      "\n",
      "Use ALBERT as text encoder. Freeze: False\n",
      "********** Enabling Positional Prompt ***************\n",
      "\n",
      "  Warming up (5 iterations)... Done\n",
      "  Running benchmark (30 iterations)... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_test_runs):\n\u001b[1;32m---> 93\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\Desktop\\HuMAR\\models\\uniphd\\uniphd.py:362\u001b[0m, in \u001b[0;36mUniPHD.forward\u001b[1;34m(self, samples, targets)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    361\u001b[0m     text_embed \u001b[38;5;241m=\u001b[39m repeat(text_sentence_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c -> b q c\u001b[39m\u001b[38;5;124m'\u001b[39m, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_queries)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m     hs_pose, refpoint_pose, mix_refpoint, mix_embedding, memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# heads\u001b[39;00m\n\u001b[0;32m    365\u001b[0m outputs_class\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\Desktop\\HuMAR\\models\\uniphd\\fully_convolutional_optim.py:663\u001b[0m, in \u001b[0;36mFullyConvolutionalTransformer.forward\u001b[1;34m(self, srcs, masks, pos_embeds, query_embed)\u001b[0m\n\u001b[0;32m    660\u001b[0m valid_ratios \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_valid_ratio(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m masks], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# Fully convolutional encoder\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_flatten\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# Two-stage query generation\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtwo_stage_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\Desktop\\HuMAR\\models\\uniphd\\fully_convolutional_optim.py:255\u001b[0m, in \u001b[0;36mFullyConvEncoder.forward\u001b[1;34m(self, src_flatten, pos, level_start_index, spatial_shapes, valid_ratios, key_padding_mask)\u001b[0m\n\u001b[0;32m    252\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m pos[:, spatial_index : spatial_index \u001b[38;5;241m+\u001b[39m h \u001b[38;5;241m*\u001b[39m w, :]\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Reshape to 2D\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m pos_embed\u001b[38;5;241m.\u001b[39mreshape(B, h, w, C)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FPS BENCHMARK - Testing inference speed for each backbone\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from util.misc import nested_tensor_from_tensor_list\n",
    "\n",
    "# Configuration\n",
    "num_warmup_runs = 5  # Warmup iterations to stabilize GPU\n",
    "num_test_runs = 30   # Number of iterations for timing\n",
    "batch_size = 1       # Images per batch\n",
    "image_size = 640     # Image resolution\n",
    "\n",
    "# Sample captions with 4-5 words each\n",
    "sample_captions = [\n",
    "    \"A person standing outside\",\n",
    "    \"Man sitting on chair\",\n",
    "    \"Woman walking in park\",\n",
    "    \"Person running with dog\",\n",
    "    \"Child playing with ball\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FPS BENCHMARK - INFERENCE SPEED TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Image size: {image_size}x{image_size}\")\n",
    "print(f\"  Warmup runs: {num_warmup_runs}\")\n",
    "print(f\"  Test runs: {num_test_runs}\")\n",
    "print(f\"  Caption length: 4-5 words\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get list of working backbones from previous test\n",
    "working_backbones = [r['backbone'] for r in results if \"SUCCESS\" in r['status']]\n",
    "print(f\"Testing {len(working_backbones)} working backbones...\\n\")\n",
    "\n",
    "fps_results = []\n",
    "\n",
    "for idx, backbone_name in enumerate(working_backbones, 1):\n",
    "    print(f\"[{idx}/{len(working_backbones)}] Testing: {backbone_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Build model with this backbone\n",
    "        args.backbone = backbone_name\n",
    "        test_model, _, _ = build_model_main_backbone(args)\n",
    "        \n",
    "        # Move to CUDA and set to eval mode\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        test_model = test_model.to(device)\n",
    "        test_model.eval()\n",
    "        \n",
    "        # Create dummy data\n",
    "        dummy_images = torch.randn(batch_size, 3, image_size, image_size).to(device)\n",
    "        \n",
    "        # Create dummy targets with captions\n",
    "        dummy_targets = []\n",
    "        for i in range(batch_size):\n",
    "            keypoints_flat = torch.rand(1, 17 * 3).to(device)\n",
    "            target = {\n",
    "                'caption': sample_captions[i % len(sample_captions)],\n",
    "                'labels': torch.tensor([1], dtype=torch.long).to(device),\n",
    "                'boxes': torch.tensor([[0.5, 0.5, 0.3, 0.4]], dtype=torch.float32).to(device),\n",
    "                'keypoints': keypoints_flat,\n",
    "                'area': torch.tensor([0.12], dtype=torch.float32).to(device),\n",
    "                'iscrowd': torch.tensor([0], dtype=torch.long).to(device),\n",
    "                'orig_size': torch.tensor([image_size, image_size], dtype=torch.long).to(device),\n",
    "                'size': torch.tensor([image_size, image_size], dtype=torch.long).to(device),\n",
    "                'scribble': torch.rand(8, 2).to(device)\n",
    "            }\n",
    "            dummy_targets.append(target)\n",
    "        \n",
    "        samples = nested_tensor_from_tensor_list(list(dummy_images))\n",
    "        \n",
    "        # Warmup runs\n",
    "        print(f\"  Warming up ({num_warmup_runs} iterations)...\", end=\" \")\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup_runs):\n",
    "                _ = test_model(samples, dummy_targets)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # Timed runs\n",
    "        print(f\"  Running benchmark ({num_test_runs} iterations)...\", end=\" \")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_test_runs):\n",
    "                _ = test_model(samples, dummy_targets)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_batch = total_time / num_test_runs\n",
    "        avg_time_per_image = avg_time_per_batch / batch_size\n",
    "        fps = 1.0 / avg_time_per_image\n",
    "        ms_per_image = avg_time_per_image * 1000\n",
    "        \n",
    "        # Get parameter count from previous results\n",
    "        params_M = next((r['params_M'] for r in results if r['backbone'] == backbone_name), 0)\n",
    "        \n",
    "        fps_results.append({\n",
    "            'backbone': backbone_name,\n",
    "            'fps': fps,\n",
    "            'ms_per_image': ms_per_image,\n",
    "            'avg_time_per_batch': avg_time_per_batch,\n",
    "            'params_M': params_M\n",
    "        })\n",
    "        \n",
    "        print(f\"  âœ… FPS: {fps:.2f}  |  {ms_per_image:.1f} ms/image\")\n",
    "        print()\n",
    "        \n",
    "        # Clean up\n",
    "        del test_model, dummy_images, dummy_targets, samples\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error: {str(e)[:100]}\")\n",
    "        print()\n",
    "        fps_results.append({\n",
    "            'backbone': backbone_name,\n",
    "            'fps': 0,\n",
    "            'ms_per_image': 0,\n",
    "            'avg_time_per_batch': 0,\n",
    "            'params_M': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FPS BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Backbone':<25} {'FPS':<10} {'ms/image':<12} {'Params (M)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in fps_results:\n",
    "    if r['fps'] > 0:\n",
    "        print(f\"{r['backbone']:<25} {r['fps']:<10.2f} {r['ms_per_image']:<12.1f} {r['params_M']:<12.2f}\")\n",
    "    else:\n",
    "        print(f\"{r['backbone']:<25} {'FAILED':<10} {'-':<12} {'-':<12}\")\n",
    "\n",
    "# Sorted by FPS (fastest first)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 10 FASTEST BACKBONES (by FPS):\")\n",
    "print(\"=\" * 80)\n",
    "valid_results = [r for r in fps_results if r['fps'] > 0]\n",
    "sorted_by_fps = sorted(valid_results, key=lambda x: x['fps'], reverse=True)\n",
    "\n",
    "for idx, r in enumerate(sorted_by_fps[:10], 1):\n",
    "    print(f\"{idx:2d}. {r['backbone']:20s}  {r['fps']:6.2f} FPS  ({r['ms_per_image']:5.1f} ms/img)  [{r['params_M']:.2f}M params]\")\n",
    "\n",
    "# Efficiency ranking (FPS per million parameters)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 10 MOST EFFICIENT (FPS per Million Parameters):\")\n",
    "print(\"=\" * 80)\n",
    "for r in valid_results:\n",
    "    if r['params_M'] > 0:\n",
    "        r['efficiency'] = r['fps'] / r['params_M']\n",
    "    else:\n",
    "        r['efficiency'] = 0\n",
    "\n",
    "sorted_by_efficiency = sorted([r for r in valid_results if r.get('efficiency', 0) > 0], \n",
    "                              key=lambda x: x['efficiency'], reverse=True)\n",
    "\n",
    "for idx, r in enumerate(sorted_by_efficiency[:10], 1):\n",
    "    print(f\"{idx:2d}. {r['backbone']:20s}  {r['efficiency']:6.2f} FPS/M  ({r['fps']:5.2f} FPS, {r['params_M']:.2f}M params)\")\n",
    "\n",
    "# Restore original backbone\n",
    "args.backbone = \"mobilevit_xxs\"\n",
    "print(f\"\\nâœ“ Benchmark complete. Restored backbone to: {args.backbone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4a1eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "==========================================================================================\n",
      "\n",
      "Rank   Backbone             Params (M)   FPS        ms/img     Efficiency  \n",
      "------------------------------------------------------------------------------------------\n",
      "1      poolformer_s12       32.96        7.50       133.2      0.23        \n",
      "2      poolformer_s24       42.43        7.29       137.1      0.17        \n",
      "3      swin_T_224_1k        49.80        7.24       138.1      0.15        \n",
      "4      poolformer_s36       51.91        7.22       138.5      0.14        \n",
      "5      mobilevit_s          22.95        7.22       138.6      0.31        \n",
      "6      mobilevit_xxs        20.71        7.06       141.7      0.34        \n",
      "7      mobilevit_xs         21.34        7.02       142.4      0.33        \n",
      "\n",
      "==========================================================================================\n",
      "RECOMMENDED CONFIGURATIONS FOR DIFFERENT SCENARIOS:\n",
      "==========================================================================================\n",
      "\n",
      "1. ðŸš€ FASTEST MODEL (Maximum Speed):\n",
      "   poolformer_s12\n",
      "   â†’ 7.50 FPS | 133.2 ms/img | 32.96M params\n",
      "\n",
      "2. ðŸ’¡ LIGHTEST MODEL (Minimum Memory):\n",
      "   mobilevit_xxs\n",
      "   â†’ 20.71M params | 7.06 FPS | 141.7 ms/img\n",
      "\n",
      "3. âš¡ MOST EFFICIENT (Best FPS/Param Ratio):\n",
      "   mobilevit_xxs\n",
      "   â†’ 0.34 FPS/M | 7.06 FPS | 20.71M params\n",
      "\n",
      "4. âš–ï¸  BALANCED (Speed + Size):\n",
      "   mobilevit_s\n",
      "   â†’ 7.22 FPS | 22.95M params | 138.6 ms/img\n",
      "\n",
      "==========================================================================================\n",
      "ðŸ’¡ RECOMMENDATION FOR RTX 4050 6GB:\n",
      "==========================================================================================\n",
      "\n",
      "Best overall: mobilevit_xxs\n",
      "  - Excellent efficiency: 0.34 FPS per Million Parameters\n",
      "  - Real-time capable: 7.06 FPS (141.7 ms/image)\n",
      "  - Lightweight: 20.71M parameters\n",
      "\n",
      "With transformer: 'fully_conv_optim' (2-3M params)\n",
      "With text encoder: 'ALBERT' (11.8M params, 768-dim)\n",
      "  â†’ Total estimated: ~35.5M parameters\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE COMPARISON: Parameters vs FPS\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create comparison dataframe-like display\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\n{'Rank':<6} {'Backbone':<20} {'Params (M)':<12} {'FPS':<10} {'ms/img':<10} {'Efficiency':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Merge results from both tests\n",
    "comparison = []\n",
    "for fps_r in fps_results:\n",
    "    if fps_r['fps'] > 0:\n",
    "        comparison.append({\n",
    "            'backbone': fps_r['backbone'],\n",
    "            'params_M': fps_r['params_M'],\n",
    "            'fps': fps_r['fps'],\n",
    "            'ms_per_image': fps_r['ms_per_image'],\n",
    "            'efficiency': fps_r['fps'] / fps_r['params_M'] if fps_r['params_M'] > 0 else 0\n",
    "        })\n",
    "\n",
    "# Sort by FPS\n",
    "comparison_sorted = sorted(comparison, key=lambda x: x['fps'], reverse=True)\n",
    "\n",
    "for idx, r in enumerate(comparison_sorted, 1):\n",
    "    print(f\"{idx:<6} {r['backbone']:<20} {r['params_M']:<12.2f} {r['fps']:<10.2f} {r['ms_per_image']:<10.1f} {r['efficiency']:<12.2f}\")\n",
    "\n",
    "# Best choices for different scenarios\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"RECOMMENDED CONFIGURATIONS FOR DIFFERENT SCENARIOS:\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Find best for each category\n",
    "fastest = max(comparison, key=lambda x: x['fps'])\n",
    "smallest = min(comparison, key=lambda x: x['params_M'])\n",
    "most_efficient = max(comparison, key=lambda x: x['efficiency'])\n",
    "\n",
    "# Find balanced (good FPS, reasonable params)\n",
    "balanced_candidates = [r for r in comparison if r['params_M'] < 30 and r['fps'] > 5]\n",
    "balanced = max(balanced_candidates, key=lambda x: x['fps']) if balanced_candidates else fastest\n",
    "\n",
    "print(f\"\\n1. ðŸš€ FASTEST MODEL (Maximum Speed):\")\n",
    "print(f\"   {fastest['backbone']}\")\n",
    "print(f\"   â†’ {fastest['fps']:.2f} FPS | {fastest['ms_per_image']:.1f} ms/img | {fastest['params_M']:.2f}M params\")\n",
    "\n",
    "print(f\"\\n2. ðŸ’¡ LIGHTEST MODEL (Minimum Memory):\")\n",
    "print(f\"   {smallest['backbone']}\")\n",
    "print(f\"   â†’ {smallest['params_M']:.2f}M params | {smallest['fps']:.2f} FPS | {smallest['ms_per_image']:.1f} ms/img\")\n",
    "\n",
    "print(f\"\\n3. âš¡ MOST EFFICIENT (Best FPS/Param Ratio):\")\n",
    "print(f\"   {most_efficient['backbone']}\")\n",
    "print(f\"   â†’ {most_efficient['efficiency']:.2f} FPS/M | {most_efficient['fps']:.2f} FPS | {most_efficient['params_M']:.2f}M params\")\n",
    "\n",
    "print(f\"\\n4. âš–ï¸  BALANCED (Speed + Size):\")\n",
    "print(f\"   {balanced['backbone']}\")\n",
    "print(f\"   â†’ {balanced['fps']:.2f} FPS | {balanced['params_M']:.2f}M params | {balanced['ms_per_image']:.1f} ms/img\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"ðŸ’¡ RECOMMENDATION FOR RTX 4050 6GB:\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nBest overall: {most_efficient['backbone']}\")\n",
    "print(f\"  - Excellent efficiency: {most_efficient['efficiency']:.2f} FPS per Million Parameters\")\n",
    "print(f\"  - Real-time capable: {most_efficient['fps']:.2f} FPS ({most_efficient['ms_per_image']:.1f} ms/image)\")\n",
    "print(f\"  - Lightweight: {most_efficient['params_M']:.2f}M parameters\")\n",
    "print(f\"\\nWith transformer: 'fully_conv_optim' (2-3M params)\")\n",
    "print(f\"With text encoder: 'ALBERT' (11.8M params, 768-dim)\")\n",
    "print(f\"  â†’ Total estimated: ~{most_efficient['params_M'] + 3 + 11.8:.1f}M parameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
